{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdc0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dharm\\anaconda3\\lib\\site-packages (4.36.0)\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\dharm\\anaconda3\\lib\\site-packages (4.6.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\dharm\\anaconda3\\lib\\site-packages (1.1.5)\n",
      "Requirement already satisfied: trio<1.0,>=0.30.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from selenium) (0.31.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: urllib3[socks]<3.0,>=2.5.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from selenium) (2.6.0)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from webdriver-manager) (1.1.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.2)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: outcome in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.4)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dharm\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Installing collected packages: requests, webdriver-manager\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "Successfully installed requests-2.32.5 webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium webdriver-manager beautifulsoup4 lxml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50511ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "JUNIPER NETWORKS ACX SERIES EOL TABLE SCRAPER\n",
      "======================================================================\n",
      "\n",
      "Loading page: https://support.juniper.net/support/eol/product/acx_series/\n",
      "Waiting 10 seconds for JavaScript to execute...\n",
      "✓ Tables detected on page\n",
      "Found 6 table(s)\n",
      "\n",
      "Processing Table 1:\n",
      "  Shape: (1, 8)\n",
      "  Columns: ['Technical Support Bulletin (TSB)', 'SKU Description', 'EOL Announced', 'Last Order', 'End of Hardware Failure Analysis', 'End of Engineering', 'Last Software Version', 'End of Support']\n",
      "\n",
      "Processing Table 2:\n",
      "  Shape: (10, 7)\n",
      "  Columns: ['Product', 'EOL Announced', 'Last Order', 'Last Date to Convert Warranty', 'Same Day Support Discontinued', 'Next Day Support Discontinued', 'End of Support']\n",
      "\n",
      "Processing Table 3:\n",
      "  Shape: (2, 3)\n",
      "  Columns: [0, 1, 2]\n",
      "\n",
      "Processing Table 4:\n",
      "  Shape: (1, 2)\n",
      "  Columns: [0, 1]\n",
      "\n",
      "Processing Table 5:\n",
      "  Shape: (1, 2)\n",
      "  Columns: [0, 1]\n",
      "\n",
      "✓ Browser closed\n",
      "\n",
      "======================================================================\n",
      "✓ EXTRACTION SUCCESSFUL!\n",
      "======================================================================\n",
      "\n",
      "✓ Saved: juniper_acx_series_table_1.csv (1 rows × 9 columns)\n",
      "✓ Saved: juniper_acx_series_table_2.csv (10 rows × 8 columns)\n",
      "✓ Saved: juniper_acx_series_table_3.csv (2 rows × 4 columns)\n",
      "✓ Saved: juniper_acx_series_table_4.csv (1 rows × 3 columns)\n",
      "✓ Saved: juniper_acx_series_table_5.csv (1 rows × 3 columns)\n",
      "\n",
      "======================================================================\n",
      "DATA PREVIEW\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Table 1: (1, 9) ---\n",
      "                            Technical Support Bulletin (TSB)                                                       Technical Support Bulletin (TSB)_url                                                                                                                                                                           SKU Description EOL Announced  Last Order End of Hardware Failure Analysis End of Engineering  Last Software Version End of Support\n",
      "0  TSB96109 - End of Life Notification: ACX Transform Series  https://supportportal.juniper.net/s/article/End-Of-Life-Notification-ACX-Transform-Series  ACX710DC, ACX700CBLMT, ACX700DC21, ACX700DC210, ACX700DC22, ACX700DC25, ACX700DC310, ACX700DC32, ACX700DC35, ACX700GND1, ACX700GND10, ACX700GND2, ACX700GND5, ACX700RACK21, ACX700RACK23    05/15/2025  11/15/2025                       11/15/2026         11/15/2028                   25.2     11/15/2030\n",
      "\n",
      "\n",
      "--- Table 2: (10, 8) ---\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Product                                                                                  Product_url EOL Announced  Last Order Last Date to Convert Warranty Same Day Support Discontinued Next Day Support Discontinued End of Support\n",
      "0  ACX500-LIC-SEC, ACX500-LIC-GPS, ACX2100-DC-BASE, ACX2100-AC-BASE, ACX-2K-LIC-SEC, ACX-2K-LIC-ADV, ACX-2K-LIC-1588, ACX-2K-LIC-10G-UPG, RACK-MNT-25RU-23-S, RACK-MNT-25RU-19-S, RACK-MNT-1RU-23-S, RACK-MNT-1RU-19-S, PWR-ACX4000-DC-S, PKG-25RU-ACXS, PKG-1U-ACXS, FLTR-KIT-ACX4000-M-S, FANTRAY-ACX4000-M-S, CHAS-ACX4000-S, BLANK-ACX4000-MIC-S, ACX-MIC-6GE-CU-SFP, ACX-MIC-4COC3-1COC12CE, ACX-MIC-16CHE1-T1-CE, ACX500-GPS, ACX500-DC, ACX500-AC, ACX4000BASE-DC, ACX4000-2-6GE-DC, ACX2200-DC, ACX2200-AC, ACX2100-DC, ACX2100-AC, ACX1100-DC, ACX1100-AC, ACX1000-DC                     https://supportportal.juniper.net/s/article/End-Of-Life-Notification-ACX    06/28/2023  06/28/2024                           NaN                    06/28/2029                    06/28/2029     06/28/2029\n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ACX6160-T-CHAS, ACX6160-T-DC, ACX6160-FAN-1RU                 https://supportportal.juniper.net/s/article/End-of-Life-Announcement-ACX6160    09/15/2021  11/30/2021                    11/30/2022                    11/20/2026                    11/20/2026     11/30/2026\n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                             ACX6360-OR-AC, ACX6360-OR-DC, ACX6360-OX-AC, ACX6360-OX-DC, JNP6360-CHAS, ACX6360-OR-AC-HW, ACX6360-OR-DC-HW, ACX6360-OX-AC-HW, ACX6360-OX-DC-HW  https://supportportal.juniper.net/s/article/EOL-Announcement-ACX6360-OR-and-ACX6360-OX-SKUS    09/01/2022  09/01/2022                    09/01/2023                    09/01/2027                    09/01/2027     09/01/2027\n",
      "\n",
      "\n",
      "--- Table 3: (2, 4) ---\n",
      "    0 0_url       1   2\n",
      "0 NaN  None  search NaN\n",
      "1 NaN  None     NaN NaN\n",
      "\n",
      "\n",
      "--- Table 4: (1, 3) ---\n",
      "    0 0_url   1\n",
      "0 NaN  None NaN\n",
      "\n",
      "\n",
      "--- Table 5: (1, 3) ---\n",
      "    0 0_url                               1\n",
      "0 NaN  None  Sort by:RelevanceRelevanceDate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Juniper Networks ACX Series EOL Table Scraper using Selenium\n",
    "Extracts tables from dynamically loaded JavaScript content\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "URL = \"https://support.juniper.net/support/eol/product/acx_series/\"\n",
    "\n",
    "def _drop_unnamed_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove unnamed columns from DataFrame\"\"\"\n",
    "    return df.loc[:, ~df.columns.astype(str).str.match(r\"^Unnamed\")]\n",
    "\n",
    "def scrape_acx_tables_selenium(url: str = URL, headless: bool = True, wait_time: int = 10):\n",
    "    \"\"\"\n",
    "    Scrape tables from Juniper Networks EOL page using Selenium\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to scrape\n",
    "        headless: Run browser in headless mode (True) or visible mode (False)\n",
    "        wait_time: Time in seconds to wait for tables to load\n",
    "    \n",
    "    Returns:\n",
    "        List of DataFrames containing the scraped tables\n",
    "    \"\"\"\n",
    "    # Setup Chrome options\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36\"\n",
    "    )\n",
    "    \n",
    "    # Initialize driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading page: {url}\")\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for tables to load\n",
    "        print(f\"Waiting {wait_time} seconds for JavaScript to execute...\")\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "        # Try to explicitly wait for table elements\n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "            )\n",
    "            print(\"✓ Tables detected on page\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: {str(e)}\")\n",
    "            print(\"Proceeding with available content...\")\n",
    "        \n",
    "        # Get page source after JavaScript execution\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        # Find all tables\n",
    "        table_nodes = soup.find_all(\"table\")\n",
    "        print(f\"Found {len(table_nodes)} table(s)\")\n",
    "        \n",
    "        if len(table_nodes) == 0:\n",
    "            print(\"\\n❌ No tables found!\")\n",
    "            print(\"The page may use a different loading mechanism.\")\n",
    "            print(\"Try increasing wait_time or check browser console for errors.\")\n",
    "            return []\n",
    "        \n",
    "        # Parse tables using pandas\n",
    "        dfs = pd.read_html(html)\n",
    "        \n",
    "        out = []\n",
    "        for i, (df, table) in enumerate(zip(dfs, table_nodes), start=1):\n",
    "            print(f\"\\nProcessing Table {i}:\")\n",
    "            print(f\"  Shape: {df.shape}\")\n",
    "            print(f\"  Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Clean DataFrame\n",
    "            df = _drop_unnamed_cols(df)\n",
    "            \n",
    "            # Extract hyperlinks from first column\n",
    "            links = []\n",
    "            rows = table.select(\"tr\")[1:]  # Skip header row\n",
    "            for tr in rows:\n",
    "                td = tr.find(\"td\")\n",
    "                a = td.find(\"a\") if td else None\n",
    "                links.append(a[\"href\"] if a and a.has_attr(\"href\") else None)\n",
    "            \n",
    "            # Align link list length with df length\n",
    "            if len(links) != len(df):\n",
    "                links = (links + [None] * len(df))[:len(df)]\n",
    "            \n",
    "            # Insert URL column after first column\n",
    "            if len(df.columns) > 0:\n",
    "                first_col = df.columns[0]\n",
    "                df.insert(1, f\"{first_col}_url\", links)\n",
    "            \n",
    "            out.append(df)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during scraping: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"\\n✓ Browser closed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"JUNIPER NETWORKS ACX SERIES EOL TABLE SCRAPER\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Run the scraper\n",
    "    tables = scrape_acx_tables_selenium(headless=True, wait_time=10)\n",
    "    \n",
    "    if tables:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"✓ EXTRACTION SUCCESSFUL!\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Save each table to CSV\n",
    "        for idx, df in enumerate(tables, start=1):\n",
    "            filename = f\"juniper_acx_series_table_{idx}.csv\"\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"✓ Saved: {filename} ({df.shape[0]} rows × {df.shape[1]} columns)\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"DATA PREVIEW\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Display preview of each table\n",
    "        for idx, df in enumerate(tables, start=1):\n",
    "            print(f\"\\n--- Table {idx}: {df.shape} ---\")\n",
    "            print(df.head(3).to_string())\n",
    "            print()\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n❌ EXTRACTION FAILED\")\n",
    "        print(\"No tables were extracted from the page.\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Increase wait_time parameter (try 15-20 seconds)\")\n",
    "        print(\"2. Run with headless=False to see what's loading\")\n",
    "        print(\"3. Check if the page requires authentication\")\n",
    "        print(\"4. Inspect browser Network tab for the actual API endpoint\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb9e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
