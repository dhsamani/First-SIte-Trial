{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644674b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium webdriver-manager beautifulsoup4 lxml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Juniper Networks ACX Series EOL Table Scraper for Databricks\n",
    "Class-based implementation with proper separation of concerns\n",
    "\"\"\"\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class JuniperEOLScraper:\n",
    "    \"\"\"\n",
    "    Web scraper class for extracting End-of-Life tables from Juniper Networks website\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url: str, wait_time: int = 10, headless: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with configuration\n",
    "        \n",
    "        Args:\n",
    "            url: Target URL to scrape\n",
    "            wait_time: Seconds to wait for page load\n",
    "            headless: Run browser in headless mode\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        self.wait_time = wait_time\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        self.tables = []\n",
    "        self.dataframes = []\n",
    "        \n",
    "    def _setup_chrome_options(self) -> Options:\n",
    "        \"\"\"\n",
    "        Configure Chrome options for Databricks environment\n",
    "        \n",
    "        Returns:\n",
    "            Configured Chrome Options object\n",
    "        \"\"\"\n",
    "        temp_dir = \"/tmp/selenium_temp\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        \n",
    "        chrome_options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            chrome_options.add_argument(\"--headless=new\")\n",
    "        \n",
    "        # Essential arguments for Databricks\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--disable-software-rasterizer\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        chrome_options.add_argument(\"--disable-setuid-sandbox\")\n",
    "        \n",
    "        # Critical: Set temp directories to /tmp\n",
    "        chrome_options.add_argument(f\"--user-data-dir={temp_dir}/user-data\")\n",
    "        chrome_options.add_argument(f\"--data-path={temp_dir}/data-path\")\n",
    "        chrome_options.add_argument(f\"--homedir={temp_dir}\")\n",
    "        chrome_options.add_argument(f\"--disk-cache-dir={temp_dir}/cache\")\n",
    "        \n",
    "        chrome_options.add_argument(\n",
    "            \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0 Safari/537.36\"\n",
    "        )\n",
    "        \n",
    "        return chrome_options\n",
    "    \n",
    "    def _initialize_driver(self):\n",
    "        \"\"\"\n",
    "        Initialize Selenium WebDriver with configured options\n",
    "        \"\"\"\n",
    "        chrome_options = self._setup_chrome_options()\n",
    "        service = Service(executable_path=\"/usr/local/bin/chromedriver\")\n",
    "        \n",
    "        print(\"Initializing Chrome WebDriver...\")\n",
    "        self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    def _load_page(self):\n",
    "        \"\"\"\n",
    "        Load the target webpage and wait for content\n",
    "        \"\"\"\n",
    "        print(f\"Loading page: {self.url}\")\n",
    "        self.driver.get(self.url)\n",
    "        \n",
    "        print(f\"Waiting {self.wait_time} seconds for JavaScript to execute...\")\n",
    "        time.sleep(self.wait_time)\n",
    "        \n",
    "        # Wait for tables to appear\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "            )\n",
    "            print(\"✓ Tables detected on page\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Warning: Timeout waiting for tables - {str(e)}\")\n",
    "            print(\"Proceeding with available content...\")\n",
    "    \n",
    "    def _extract_tables_html(self) -> List:\n",
    "        \"\"\"\n",
    "        Extract table HTML elements from the page\n",
    "        \n",
    "        Returns:\n",
    "            List of BeautifulSoup table elements\n",
    "        \"\"\"\n",
    "        html = self.driver.page_source\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        \n",
    "        table_nodes = soup.find_all(\"table\")\n",
    "        print(f\"Found {len(table_nodes)} table(s)\")\n",
    "        \n",
    "        return table_nodes\n",
    "    \n",
    "    def _extract_hyperlinks(self, table) -> List[Optional[str]]:\n",
    "        \"\"\"\n",
    "        Extract hyperlinks from the first column of a table\n",
    "        \n",
    "        Args:\n",
    "            table: BeautifulSoup table element\n",
    "            \n",
    "        Returns:\n",
    "            List of URLs or None values\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        rows = table.select(\"tr\")[1:]  # Skip header row\n",
    "        \n",
    "        for tr in rows:\n",
    "            td = tr.find(\"td\")\n",
    "            a = td.find(\"a\") if td else None\n",
    "            links.append(a[\"href\"] if a and a.has_attr(\"href\") else None)\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    @staticmethod\n",
    "    def _drop_unnamed_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove unnamed columns from DataFrame\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with unnamed columns removed\n",
    "        \"\"\"\n",
    "        return df.loc[:, ~df.columns.astype(str).str.match(r\"^Unnamed\")]\n",
    "    \n",
    "    def scrape_tables(self) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Main scraping method - orchestrates the entire scraping process\n",
    "        \n",
    "        Returns:\n",
    "            List of pandas DataFrames containing scraped table data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize driver\n",
    "            self._initialize_driver()\n",
    "            \n",
    "            # Load page\n",
    "            self._load_page()\n",
    "            \n",
    "            # Extract table HTML\n",
    "            table_nodes = self._extract_tables_html()\n",
    "            \n",
    "            if len(table_nodes) == 0:\n",
    "                print(\"\\n❌ No tables found on the page!\")\n",
    "                return []\n",
    "            \n",
    "            # Parse tables with pandas\n",
    "            html = self.driver.page_source\n",
    "            dfs = pd.read_html(html)\n",
    "            \n",
    "            # Process each table\n",
    "            self.dataframes = []\n",
    "            for i, (df, table) in enumerate(zip(dfs, table_nodes), start=1):\n",
    "                print(f\"\\nProcessing Table {i}: {df.shape}\")\n",
    "                \n",
    "                # Clean DataFrame\n",
    "                df = self._drop_unnamed_cols(df)\n",
    "                \n",
    "                # Extract hyperlinks\n",
    "                links = self._extract_hyperlinks(table)\n",
    "                \n",
    "                # Align link list length with df length\n",
    "                if len(links) != len(df):\n",
    "                    links = (links + [None] * len(df))[:len(df)]\n",
    "                \n",
    "                # Insert URL column after first column\n",
    "                if len(df.columns) > 0:\n",
    "                    first_col = df.columns[0]\n",
    "                    df.insert(1, f\"{first_col}_url\", links)\n",
    "                \n",
    "                self.dataframes.append(df)\n",
    "            \n",
    "            print(f\"\\n✓ Successfully extracted {len(self.dataframes)} table(s)\")\n",
    "            return self.dataframes\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Error during scraping: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return []\n",
    "            \n",
    "        finally:\n",
    "            self._cleanup()\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"\n",
    "        Clean up resources and close browser\n",
    "        \"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            print(\"\\n✓ Browser closed\")\n",
    "    \n",
    "    def display_tables(self):\n",
    "        \"\"\"\n",
    "        Display all extracted tables using Databricks display()\n",
    "        \"\"\"\n",
    "        if not self.dataframes:\n",
    "            print(\"No tables to display. Run scrape_tables() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"DATA PREVIEW\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        for idx, df in enumerate(self.dataframes, start=1):\n",
    "            print(f\"\\n--- Table {idx}: {df.shape} ---\")\n",
    "            display(df.head(10))\n",
    "    \n",
    "    def save_to_csv(self, output_path: str = \"/dbfs/tmp\", prefix: str = \"juniper_acx_table\"):\n",
    "        \"\"\"\n",
    "        Save all extracted tables to CSV files\n",
    "        \n",
    "        Args:\n",
    "            output_path: Directory path to save CSV files\n",
    "            prefix: Filename prefix for CSV files\n",
    "        \"\"\"\n",
    "        if not self.dataframes:\n",
    "            print(\"No tables to save. Run scrape_tables() first.\")\n",
    "            return\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        \n",
    "        saved_files = []\n",
    "        for idx, df in enumerate(self.dataframes, start=1):\n",
    "            filename = f\"{prefix}_{idx}.csv\"\n",
    "            filepath = os.path.join(output_path, filename)\n",
    "            \n",
    "            df.to_csv(filepath, index=False)\n",
    "            saved_files.append(filepath)\n",
    "            print(f\"✓ Saved Table {idx}: {filepath} ({df.shape[0]} rows × {df.shape[1]} columns)\")\n",
    "        \n",
    "        return saved_files\n",
    "    \n",
    "    def get_dataframes(self) -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the list of extracted DataFrames\n",
    "        \n",
    "        Returns:\n",
    "            List of pandas DataFrames\n",
    "        \"\"\"\n",
    "        return self.dataframes\n",
    "    \n",
    "    def get_summary(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get summary statistics of extracted tables\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with summary information\n",
    "        \"\"\"\n",
    "        if not self.dataframes:\n",
    "            return {\"status\": \"No tables extracted\"}\n",
    "        \n",
    "        summary = {\n",
    "            \"total_tables\": len(self.dataframes),\n",
    "            \"tables\": []\n",
    "        }\n",
    "        \n",
    "        for idx, df in enumerate(self.dataframes, start=1):\n",
    "            summary[\"tables\"].append({\n",
    "                \"table_number\": idx,\n",
    "                \"shape\": df.shape,\n",
    "                \"columns\": list(df.columns),\n",
    "                \"rows\": len(df),\n",
    "                \"columns_count\": len(df.columns)\n",
    "            })\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"JUNIPER NETWORKS ACX SERIES EOL TABLE SCRAPER - DATABRICKS\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Configuration\n",
    "    URL = \"https://support.juniper.net/support/eol/product/acx_series/\"\n",
    "    OUTPUT_PATH = \"/dbfs/tmp\"\n",
    "    FILE_PREFIX = \"juniper_acx_series_table\"\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = JuniperEOLScraper(\n",
    "        url=URL,\n",
    "        wait_time=10,\n",
    "        headless=True\n",
    "    )\n",
    "    \n",
    "    # Scrape tables\n",
    "    print(\"Starting scraping process...\\n\")\n",
    "    tables = scraper.scrape_tables()\n",
    "    \n",
    "    if tables:\n",
    "        # Display summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"✓ EXTRACTION SUCCESSFUL!\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        summary = scraper.get_summary()\n",
    "        print(f\"Total tables extracted: {summary['total_tables']}\\n\")\n",
    "        \n",
    "        for table_info in summary['tables']:\n",
    "            print(f\"Table {table_info['table_number']}: \"\n",
    "                  f\"{table_info['rows']} rows × {table_info['columns_count']} columns\")\n",
    "            print(f\"  Columns: {', '.join(table_info['columns'][:5])}...\")\n",
    "            print()\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SAVING TO CSV FILES\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        saved_files = scraper.save_to_csv(\n",
    "            output_path=OUTPUT_PATH,\n",
    "            prefix=FILE_PREFIX\n",
    "        )\n",
    "        \n",
    "        # Display tables\n",
    "        scraper.display_tables()\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"✓ PROCESS COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ EXTRACTION FAILED - No tables found\")\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Verify the URL is accessible\")\n",
    "        print(\"2. Increase wait_time in scraper initialization\")\n",
    "        print(\"3. Check if Chrome and ChromeDriver are properly installed\")\n",
    "        print(\"4. Run with headless=False to debug visually\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
